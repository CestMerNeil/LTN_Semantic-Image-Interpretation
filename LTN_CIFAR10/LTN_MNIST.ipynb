{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import ltn\n",
    "\n",
    "batch_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = self.fc1(x)\n",
    "        #if training:\n",
    "            #x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x             \n",
    "\n",
    "class LogitsToPredicate(nn.Module):\n",
    "    def __init__(self, logits_model):\n",
    "        super(LogitsToPredicate, self).__init__()\n",
    "        self.logits_model = logits_model\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, l, training=False):\n",
    "        logits = self.logits_model(x, training=training)\n",
    "        probs = self.softmax(logits)\n",
    "        out = torch.sum(probs * l, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = LeNet()\n",
    "P = ltn.Predicate(LogitsToPredicate(lenet))\n",
    "\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()\n",
    "\n",
    "l_0 = ltn.Constant(torch.tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
    "l_1 = ltn.Constant(torch.tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
    "l_2 = ltn.Constant(torch.tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))\n",
    "l_3 = ltn.Constant(torch.tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))\n",
    "l_4 = ltn.Constant(torch.tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0]))\n",
    "l_5 = ltn.Constant(torch.tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))\n",
    "l_6 = ltn.Constant(torch.tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))\n",
    "l_7 = ltn.Constant(torch.tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0]))\n",
    "l_8 = ltn.Constant(torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))\n",
    "l_9 = ltn.Constant(torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_sat_level(loader):\n",
    "    mean_sat = 0\n",
    "    for data, label in loader:\n",
    "        x_0 = ltn.Variable(\"x_0\", data[label == 0])\n",
    "        x_1 = ltn.Variable(\"x_1\", data[label == 1])\n",
    "        x_2 = ltn.Variable(\"x_2\", data[label == 2])\n",
    "        x_3 = ltn.Variable(\"x_3\", data[label == 3])\n",
    "        x_4 = ltn.Variable(\"x_4\", data[label == 4])\n",
    "        x_5 = ltn.Variable(\"x_5\", data[label == 5])\n",
    "        x_6 = ltn.Variable(\"x_6\", data[label == 6])\n",
    "        x_7 = ltn.Variable(\"x_7\", data[label == 7])\n",
    "        x_8 = ltn.Variable(\"x_8\", data[label == 8])\n",
    "        x_9 = ltn.Variable(\"x_9\", data[label == 9])\n",
    "        mean_sat += SatAgg(\n",
    "            Forall(x_0, P(x_0, l_0)),\n",
    "            Forall(x_1, P(x_1, l_1)),\n",
    "            Forall(x_2, P(x_2, l_2)),\n",
    "            Forall(x_3, P(x_3, l_3)),\n",
    "            Forall(x_4, P(x_4, l_4)),\n",
    "            Forall(x_5, P(x_5, l_5)),\n",
    "            Forall(x_6, P(x_6, l_6)),\n",
    "            Forall(x_7, P(x_7, l_7)),\n",
    "            Forall(x_8, P(x_8, l_8)),\n",
    "            Forall(x_9, P(x_9, l_9))\n",
    "        )\n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat\n",
    "\n",
    "def compute_acc(loader):\n",
    "    mean_acc = 0\n",
    "    for data, label in loader:\n",
    "        predictioins = lenet(data).detach().numpy()\n",
    "        predictioins = np.argmax(predictioins, axis=1)\n",
    "        mean_acc += accuracy_score(label, predictioins)\n",
    "    \n",
    "    return mean_acc / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(P.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 0.859576694170634 - Train SAT: 0.24459140002727509 - Test SAT: 0.24902211129665375 - Test Acc: 0.4899931150903392\n",
      "Epoch 1 - Train Loss: 0.5942545255025228 - Train SAT: 0.5663737654685974 - Test SAT: 0.5763744115829468 - Test Acc: 0.8042438698377582\n",
      "Epoch 2 - Train Loss: 0.39217299620310464 - Train SAT: 0.6659346222877502 - Test SAT: 0.6777152419090271 - Test Acc: 0.8814275961006638\n",
      "Epoch 3 - Train Loss: 0.32418870528539023 - Train SAT: 0.709102213382721 - Test SAT: 0.7165148854255676 - Test Acc: 0.9080727726308998\n",
      "Epoch 4 - Train Loss: 0.2860700766245524 - Train SAT: 0.7403632998466492 - Test SAT: 0.748497486114502 - Test Acc: 0.9302902896847346\n",
      "Epoch 5 - Train Loss: 0.25621575911839806 - Train SAT: 0.767669141292572 - Test SAT: 0.775672435760498 - Test Acc: 0.9446434538624632\n",
      "Epoch 6 - Train Loss: 0.22957884470621745 - Train SAT: 0.7909060716629028 - Test SAT: 0.8014370799064636 - Test Acc: 0.95149091422382\n",
      "Epoch 7 - Train Loss: 0.20888715585072834 - Train SAT: 0.8099938631057739 - Test SAT: 0.8202820420265198 - Test Acc: 0.9593833540744838\n",
      "Epoch 8 - Train Loss: 0.19122063318888347 - Train SAT: 0.8219136595726013 - Test SAT: 0.8300999999046326 - Test Acc: 0.9667810310656342\n",
      "Epoch 9 - Train Loss: 0.1776840051015218 - Train SAT: 0.8337722420692444 - Test SAT: 0.8409934043884277 - Test Acc: 0.9705129977876106\n",
      "Epoch 10 - Train Loss: 0.16921759446461995 - Train SAT: 0.8427847623825073 - Test SAT: 0.8503664135932922 - Test Acc: 0.974635301438053\n",
      "Epoch 11 - Train Loss: 0.16110219558080038 - Train SAT: 0.8462045192718506 - Test SAT: 0.8534877896308899 - Test Acc: 0.9755304837297197\n",
      "Epoch 12 - Train Loss: 0.15332833528518677 - Train SAT: 0.858292281627655 - Test SAT: 0.8681718707084656 - Test Acc: 0.980277182429941\n",
      "Epoch 13 - Train Loss: 0.147454833984375 - Train SAT: 0.8602926731109619 - Test SAT: 0.8668714165687561 - Test Acc: 0.9809815173303834\n",
      "Epoch 14 - Train Loss: 0.14398660262425741 - Train SAT: 0.8660062551498413 - Test SAT: 0.8702940344810486 - Test Acc: 0.980130986126475\n",
      "Epoch 15 - Train Loss: 0.13890787363052368 - Train SAT: 0.871257483959198 - Test SAT: 0.8771907687187195 - Test Acc: 0.9826040802452064\n",
      "Epoch 16 - Train Loss: 0.13515748182932535 - Train SAT: 0.8747474551200867 - Test SAT: 0.8782625794410706 - Test Acc: 0.982230307429941\n",
      "Epoch 17 - Train Loss: 0.13107478221257526 - Train SAT: 0.8736165761947632 - Test SAT: 0.876605212688446 - Test Acc: 0.9812753502949852\n",
      "Epoch 18 - Train Loss: 0.13028052647908528 - Train SAT: 0.8730478286743164 - Test SAT: 0.8735812306404114 - Test Acc: 0.9817204208148967\n",
      "Epoch 19 - Train Loss: 0.12732978264490763 - Train SAT: 0.8825076818466187 - Test SAT: 0.881249725818634 - Test Acc: 0.9847516535306048\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, label) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        x_0 = ltn.Variable(\"x_0\", data[label == 0])\n",
    "        x_1 = ltn.Variable(\"x_1\", data[label == 1])\n",
    "        x_2 = ltn.Variable(\"x_2\", data[label == 2])\n",
    "        x_3 = ltn.Variable(\"x_3\", data[label == 3])\n",
    "        x_4 = ltn.Variable(\"x_4\", data[label == 4])\n",
    "        x_5 = ltn.Variable(\"x_5\", data[label == 5])\n",
    "        x_6 = ltn.Variable(\"x_6\", data[label == 6])\n",
    "        x_7 = ltn.Variable(\"x_7\", data[label == 7])\n",
    "        x_8 = ltn.Variable(\"x_8\", data[label == 8])\n",
    "        x_9 = ltn.Variable(\"x_9\", data[label == 9])\n",
    "        sat_agg = SatAgg(\n",
    "            Forall(x_0, P(x_0, l_0, training=True)),\n",
    "            Forall(x_1, P(x_1, l_1, training=True)),\n",
    "            Forall(x_2, P(x_2, l_2, training=True)),\n",
    "            Forall(x_3, P(x_3, l_3, training=True)),\n",
    "            Forall(x_4, P(x_4, l_4, training=True)),\n",
    "            Forall(x_5, P(x_5, l_5, training=True)),\n",
    "            Forall(x_6, P(x_6, l_6, training=True)),\n",
    "            Forall(x_7, P(x_7, l_7, training=True)),\n",
    "            Forall(x_8, P(x_8, l_8, training=True)),\n",
    "            Forall(x_9, P(x_9, l_9, training=True))\n",
    "        )\n",
    "\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(trainloader)\n",
    "\n",
    "    print(f\"Epoch {epoch} - Train Loss: {train_loss} - Train SAT: {compute_sat_level(trainloader)} - Test SAT: {compute_sat_level(testloader)} - Test Acc: {compute_acc(testloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth tensor([1, 3, 5,  ..., 0, 8, 1])\n",
      "prediction [1 3 5 ... 0 8 1]\n"
     ]
    }
   ],
   "source": [
    "lenet.eval()\n",
    "with torch.no_grad():\n",
    "    data, label = next(iter(testloader))\n",
    "    reslut = lenet(data).detach().numpy()\n",
    "    print(\"ground truth\", label)\n",
    "    print(\"prediction\", np.argmax(reslut, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LTN_Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
